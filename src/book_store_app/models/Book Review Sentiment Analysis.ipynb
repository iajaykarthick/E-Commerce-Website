{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd52903",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m \n",
      "File \u001b[0;32m~/miniconda/envs/idmp_proj_env/lib/python3.9/site-packages/sklearn/__init__.py:82\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _distributor_init  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __check_build  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     85\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    129\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda/envs/idmp_proj_env/lib/python3.9/site-packages/sklearn/base.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     _DEFAULT_TAGS,\n\u001b[1;32m     20\u001b[0m     _safe_tags,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n",
      "File \u001b[0;32m~/miniconda/envs/idmp_proj_env/lib/python3.9/site-packages/sklearn/utils/__init__.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmurmurhash\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m murmurhash3_32\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0cf92522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d12f4149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reviews_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBook_reviews.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m reviews_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "reviews_df = pd.read_csv('Book_reviews.csv')\n",
    "reviews_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1affea",
   "metadata": {},
   "source": [
    "## Data Preprocessing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5fef66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>This book was the very first bookmobile book I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>When I read the description for this book, I c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>I just had to edit this review. This book is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                         reviewText\n",
       "0       5  This book was the very first bookmobile book I...\n",
       "1       1  When I read the description for this book, I c...\n",
       "2       5  I just had to edit this review. This book is a..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = reviews_df.iloc[:,[1,2]]\n",
    "reviews_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d751d517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    3000\n",
       "4    3000\n",
       "1    2000\n",
       "3    2000\n",
       "2    2000\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddfa7a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.loc[(reviews_df['rating'] == 1) | (reviews_df['rating'] == 2), 'rating'] = 'Bad'\n",
    "reviews_df.loc[reviews_df['rating'] == 3, 'rating'] = 'Average'\n",
    "reviews_df.loc[(reviews_df['rating'] == 4) | (reviews_df['rating'] == 5), 'rating'] = 'Good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "599c9c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Good       6000\n",
       "Bad        4000\n",
       "Average    2000\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e91724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good</td>\n",
       "      <td>This book was the very first bookmobile book I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bad</td>\n",
       "      <td>When I read the description for this book, I c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good</td>\n",
       "      <td>I just had to edit this review. This book is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>I don't normally buy 'mystery' novels because ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good</td>\n",
       "      <td>This isn't the kind of book I normally read, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>Bad</td>\n",
       "      <td>Had to read certain passages twice--typos.  Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>Average</td>\n",
       "      <td>Not what i expected. yet a very interesting bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>Good</td>\n",
       "      <td>Dragon Knights is a world where Knights ride d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>Good</td>\n",
       "      <td>Since this story is very short, it's hard to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>Good</td>\n",
       "      <td>from 1922 an amazing collection of info on sym...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rating                                         reviewText\n",
       "0         Good  This book was the very first bookmobile book I...\n",
       "1          Bad  When I read the description for this book, I c...\n",
       "2         Good  I just had to edit this review. This book is a...\n",
       "3         Good  I don't normally buy 'mystery' novels because ...\n",
       "4         Good  This isn't the kind of book I normally read, a...\n",
       "...        ...                                                ...\n",
       "11995      Bad  Had to read certain passages twice--typos.  Wi...\n",
       "11996  Average  Not what i expected. yet a very interesting bo...\n",
       "11997     Good  Dragon Knights is a world where Knights ride d...\n",
       "11998     Good  Since this story is very short, it's hard to s...\n",
       "11999     Good  from 1922 an amazing collection of info on sym...\n",
       "\n",
       "[12000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13dff0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filtering_text(text):\n",
    "filtered_text=[]\n",
    "stemming=PorterStemmer()\n",
    "for i in range(len(reviews_df)):\n",
    "    sentence_preprocessing = re.sub('[^a-zA-z]',' ',reviews_df.reviewText[i])\n",
    "    sentence_preprocessing = sentence_preprocessing.lower()\n",
    "    sentence_preprocessing = sentence_preprocessing.split()\n",
    "\n",
    "    sec_processed=[]\n",
    "    for word in sentence_preprocessing:\n",
    "        if word not in stopwords.words('english'): # to remove puntuations \n",
    "            stem_word=stemming.stem(word)\n",
    "            sec_processed.append(stem_word)\n",
    "\n",
    "\n",
    "    sentence_preprocessing = ' '.join(sec_processed)    \n",
    "    filtered_text.append(sentence_preprocessing)\n",
    "\n",
    "#    filtered_text_2.append(sentence_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20487f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_text_2[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23074f3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book first bookmobil book bought school book club love stori bet dollar donut love memori serv bought book th grade would look forward reliv memori'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00a1b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['CLeaned_Reviewes'] = filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a41a335",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from 1922 an amazing collection of info on symbols from cultures around the world and zodiac signs and gems.  while there are some ocr errors, there are also a very comprehensive linked table of contents and index.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.iloc[-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "613676ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           Good\n",
       "1            Bad\n",
       "2           Good\n",
       "3           Good\n",
       "4           Good\n",
       "          ...   \n",
       "11995        Bad\n",
       "11996    Average\n",
       "11997       Good\n",
       "11998       Good\n",
       "11999       Good\n",
       "Name: rating, Length: 12000, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32d02a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(reviews_df['CLeaned_Reviewes'], reviews_df['rating'],test_size=0.2,random_state=10,stratify=reviews_df['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d119caac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating \n",
       "Good       4800\n",
       "Bad        3200\n",
       "Average    1600\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff28c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer= TfidfVectorizer(input = 'english')\n",
    "x_train_transformed = vectorizer.fit_transform(x_train)\n",
    "x_test_transformed = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c074428f",
   "metadata": {},
   "source": [
    "## Modeling Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79969764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=250)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "model = RandomForestClassifier(n_estimators=300)\n",
    "model.fit(x_train_transformed,y_train)\n",
    "\n",
    "\n",
    "model2 = LogisticRegression(max_iter = 250)\n",
    "model2.fit(x_train_transformed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6499870",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test_transformed)\n",
    "y_pred2 = model2.predict(x_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59774516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3f7ca7c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7020833333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   5,  117,  278],\n",
       "       [   1,  565,  234],\n",
       "       [   0,   85, 1115]], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de0789d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  71  129  200]\n",
      " [  33  625  142]\n",
      " [  31   89 1080]]\n"
     ]
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred2)\n",
    "print(confusion_matrix(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfb8b5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TFDF_Transformation']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving the transformaion\n",
    "import joblib\n",
    "joblib.dump(vectorizer, 'TFDF_Transformation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bc962a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RandomForest_model']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SAving the model \n",
    "# joblib.dump(model2,'logistic_regression_model')\n",
    "# joblib.dump(model,'RandomForest_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c18b0b9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book is very bad\n"
     ]
    }
   ],
   "source": [
    "a = \"This book is very bad\"\n",
    "a = re.sub('[^a-zA-z]',' ',a)\n",
    "print((a))\n",
    "b = vectorizer.transform([a]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e6b2774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bad'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "397d1570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bad'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98835c19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14e7aaf6",
   "metadata": {},
   "source": [
    "## Code for the project part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0929e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ajaykarthicksenthilkumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b54d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51684c49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reviews_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m filtered_text\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      3\u001b[0m stemming\u001b[38;5;241m=\u001b[39mPorterStemmer()\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mreviews_df\u001b[49m)):\n\u001b[1;32m      5\u001b[0m     sentence_preprocessing \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-z]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,reviews_df\u001b[38;5;241m.\u001b[39mreviewText[i])\n\u001b[1;32m      6\u001b[0m     sentence_preprocessing \u001b[38;5;241m=\u001b[39m sentence_preprocessing\u001b[38;5;241m.\u001b[39mlower()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reviews_df' is not defined"
     ]
    }
   ],
   "source": [
    "# def filtering_text(text):\n",
    "filtered_text=[]\n",
    "stemming=PorterStemmer()\n",
    "for i in range(len(reviews_df)):\n",
    "    sentence_preprocessing = re.sub('[^a-zA-z]',' ',reviews_df.reviewText[i])\n",
    "    sentence_preprocessing = sentence_preprocessing.lower()\n",
    "    sentence_preprocessing = sentence_preprocessing.split()\n",
    "\n",
    "    sec_processed=[]\n",
    "    for word in sentence_preprocessing:\n",
    "        if word not in stopwords.words('english'): # to remove puntuations \n",
    "            stem_word=stemming.stem(word)\n",
    "            sec_processed.append(stem_word)\n",
    "\n",
    "\n",
    "    sentence_preprocessing = ' '.join(sec_processed)    \n",
    "    filtered_text.append(sentence_preprocessing)\n",
    "\n",
    "#    filtered_text_2.append(sentence_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b23c00d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy.sparse._csr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic_regression_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m TFIDF_vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTFDF_Transformation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m stemming\u001b[38;5;241m=\u001b[39mPorterStemmer()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprediction\u001b[39m(text):\n",
      "File \u001b[0;32m~/miniconda/envs/idmp_proj_env/lib/python3.9/site-packages/joblib/numpy_pickle.py:587\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m--> 587\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/miniconda/envs/idmp_proj_env/lib/python3.9/site-packages/joblib/numpy_pickle.py:506\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    504\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 506\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[1;32m    508\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[1;32m    512\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/idmp_proj_env/lib/python3.9/pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1212\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda/envs/idmp_proj_env/lib/python3.9/pickle.py:1537\u001b[0m, in \u001b[0;36m_Unpickler.load_stack_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTACK_GLOBAL requires str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda/envs/idmp_proj_env/lib/python3.9/pickle.py:1579\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1577\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING:\n\u001b[1;32m   1578\u001b[0m         module \u001b[38;5;241m=\u001b[39m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING[module]\n\u001b[0;32m-> 1579\u001b[0m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m   1581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _getattribute(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy.sparse._csr'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import re\n",
    "model = joblib.load('logistic_regression_model')\n",
    "TFIDF_vectorizer = joblib.load('TFDF_Transformation')\n",
    "stemming=PorterStemmer()\n",
    "\n",
    "def prediction(text):\n",
    "    text = re.sub('[^a-zA-z]', ' ', text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.split()\n",
    "\n",
    "    new_text=[]\n",
    "    \n",
    "    for word in text:\n",
    "        if word not in stopwords.words('english'): # to remove puntuations \n",
    "            stem_word=stemming.stem(word)\n",
    "            new_text.append(stem_word)\n",
    "    \n",
    "\n",
    "    new_text = ' '.join(new_text) \n",
    "    \n",
    "    transformed_text = TFIDF_vectorizer.transform([new_text])\n",
    "    \n",
    "    prediction = model.predict(transformed_text)\n",
    "    \n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "608b6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d245f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9c4eae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'csr_matrix' and 'csr_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m TFIDF_vectorizer \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTFDF_Transformation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mTFIDF_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhello world\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/idmp_proj_env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2102\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2099\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2101\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[0;32m-> 2102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/idmp_proj_env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1677\u001b[0m, in \u001b[0;36mTfidfTransformer.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1674\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m, attributes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midf_\u001b[39m\u001b[38;5;124m\"\u001b[39m], msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midf vector is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;66;03m# *= doesn't work\u001b[39;00m\n\u001b[0;32m-> 1677\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_idf_diag\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm:\n\u001b[1;32m   1680\u001b[0m     X \u001b[38;5;241m=\u001b[39m normalize(X, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'csr_matrix' and 'csr_matrix'"
     ]
    }
   ],
   "source": [
    "TFIDF_vectorizer = joblib.load('TFDF_Transformation')\n",
    "TFIDF_vectorizer.transform(['hello world'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
